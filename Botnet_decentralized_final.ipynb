{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some reason the context is already created when using the kernel\n",
    "conf = (SparkConf()\n",
    "           .setMaster(\"local\")\n",
    "           .setAppName(\"assignment\")\n",
    "           .set(\"spark.executor.memory\", \"1g\"))\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file PROTOTYPE\n",
    "# rdd_initial = sc.textFile(\"../alumno/Downloads/botnet_tot_syn_l.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD_Xy_raw will be like:\n",
    "#    {[\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"l\"],\n",
    "#     [\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"l\"],\n",
    "#     ...\n",
    "#     [\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"l\"]}\n",
    "\n",
    "#RDD_Xy will be like:\n",
    "#    {[(\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\"),(\"l\")],\n",
    "#     [(\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\"),(\"l\")],\n",
    "#     ...\n",
    "#     [(\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\",\"v\"),(\"l\")]}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(fileName):\n",
    "    \n",
    "    loaded_data=sc.textFile(filename)\n",
    "    # take the loaded data and separate the different values (separated by a comma)\n",
    "    RDD_Xy_raw=loaded_data.map(lambda x : np.array([i for i in x.split(',')],np.float32))\n",
    "    # cast the label to an integer value\n",
    "    RDD_Xy = RDD_Xy_raw.map(lambda y: (y[0:-1],[int(y[-1])]))\n",
    "    return RDD_Xy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(RDD_Xy):   \n",
    "   \n",
    "    # Count the number of samples\n",
    "    n_rows=RDD_Xy.count()\n",
    "    #Extract only data and \"cut\" the label\n",
    "    RDD_X=RDD_Xy.map(lambda x : x[0])\n",
    "    \n",
    "    #CALCULATE MEAN\n",
    "    #start by calculating the summation of all the values\n",
    "    RDD_summation = RDD_X.reduce(lambda x,y: x + y)\n",
    "    #divide each summation by the number of samples\n",
    "    mean = RDD_summation/n_rows\n",
    "    \n",
    "    #CALCULATE STANDARD DEVIATION\n",
    "    #start by calculating the difference between the i-th sample and the mean\n",
    "    RDD_differences = RDD_X.map(lambda x: np.subtract(x,mean))\n",
    "    #compute the square of each of the differences\n",
    "    RDD_squared = RDD_differences.map(lambda x: np.power(x,2))\n",
    "    #calcualte the summation of all the values\n",
    "    RDD_summation1 = RDD_squared.reduce(lambda x,y: np.add(x,y))\n",
    "    #calculate the variance by dividing the summation by the number of samples\n",
    "    var = RDD_summation1/n_rows\n",
    "    #the standard deviation is given by the square root of the variance\n",
    "    std = np.sqrt(var)\n",
    "    \n",
    "    \n",
    "    # Normalize the data as (X-mean)/std\n",
    "    RDD_Xy_norm = RDD_Xy.map(lambda x: ((x[0]-mean)/std,x[1]))   \n",
    "    return RDD_Xy_norm\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(RDD_Xy, iterations, learning_rate):\n",
    "    \n",
    "    \n",
    "    num_rows = RDD_Xy.count()\n",
    "    w = np.random.rand(len(RDD_Xy.take(1)[0][0]))\n",
    "    b=np.random.randint(0,100)/100\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        \n",
    "        #we need to be able to compare the predicted output with the actual output, in order to check the performance\n",
    "        #Map in order to have (x,[y,y_hat])\n",
    "        RDD_Xy_pred=RDD_Xy.map(lambda x: (x[0],x[1]+[predict(w,b,x[0])]))\n",
    "        \n",
    "        \n",
    "        # CALCULATE LOSS FUNCTION\n",
    "    \n",
    "        #in order to calculate the loss function, we need to compute log(y_hat_i) and log(1 - y_hat_i)\n",
    "        #we want to obtain a tuple like (y,[log(y_hat_i),log(1 - y_hat_i)])\n",
    "        loss_data = RDD_Xy_pred.map(lambda x: (x[1][0],[x[1][1]+epsilon,1-x[1][1]+epsilon]))\n",
    "        #Calculate the loss value for each sample applyin the formula:\n",
    "        # LOSS = -1/m * summation[(y * log(y_hat)) + ((1 - y) * log(1 - y_hat))]\n",
    "        #start by calculating summation[(y * log(y_hat)) + ((1 - y) * log(1 - y_hat))]\n",
    "        summation_loss = loss_data.map(lambda x: x[0]*np.log(x[1][0])+(1-x[0])*np.log(x[1][1]))\n",
    "        # divide by the number of samples obtaining the loss function\n",
    "        loss= -(summation_loss.reduce(lambda x,y:x+y)) / num_rows\n",
    "       \n",
    "    \n",
    "        # NOTICE:\n",
    "        # in order to calculate the derivatives we applied the given function\n",
    "        # (1/m) * SUMMATION[(y_hat - y) * x_1]\n",
    "        \n",
    "    \n",
    "        # CALCULATE WEIGHT DERIVATIVE\n",
    "        \n",
    "        #start by calculating (y_hat - y)*x for every row\n",
    "        mult_diff_w = RDD_Xy_pred.map(lambda x: np.multiply(x[0],(x[1][1]-x[1][0])))\n",
    "        #divide by the number of rows in order to obtain the derivative\n",
    "        weight_der = (mult_diff_w.reduce(lambda x,y:x+y)) / num_rows\n",
    "    \n",
    "    \n",
    "        # CALCULATE BIAS DERIVATIVE\n",
    "        \n",
    "        #start by calculating the difference (y_hat-y) for every row\n",
    "        diff_b = RDD_Xy_pred.map(lambda x: x[1][1]-x[1][0])\n",
    "        #divide by the number of samples in order to obtain the derivative\n",
    "        bias_der = (diff_b.reduce(lambda x,y:x+y)) / num_rows\n",
    "    \n",
    "        \n",
    "        # UPDATE THE VALUES OF WEIGHT AND BIAS\n",
    "        \n",
    "        #the values are updated according to the given formulas:\n",
    "        # new = old - (learning rate * derivative)   \n",
    "        w = w -learning_rate * weight_der\n",
    "        b = b -learning_rate * bias_der\n",
    "        \n",
    "        #COMPUTE THE VALUE OF THE ACCURACY (done here in order to have the updated value at each iternation)\n",
    "        acc=accuracy(w,b,RDD_Xy_pred)\n",
    "\n",
    "        #PRINT THE epoch, THE loss value AND THE accuracy value\n",
    "        print(\"EPOCH\",i,\" ---->  LOSS: \",loss,\" ACCURACY: \",acc)\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w, b, RDD_Xy):\n",
    "    \n",
    "    \n",
    "    # RDD_Xy has shape (x, [y, y_hat])\n",
    "    \n",
    "    #in this function we need to compare the predicted labels with the given ones.\n",
    "    #in order to make the comparison possible we set all the predicted labels that are under 0.5 to 0, while the others are set to 1  \n",
    "    RDD_Xy_adjusted = RDD_Xy.map(lambda x: (x[1][0],0) if x[1][1] <= 0.5 else (x[1][0],1))\n",
    "    #we count the correct predictions assigning a label 1 to the predicted values matching the given values, 0 otherwise\n",
    "    RDD_check_result= RDD_Xy_adjusted.map(lambda x: 1 if x[0] == x[1] else 0)\n",
    "    \n",
    "    n_rows = RDD_Xy_adjusted.count()\n",
    "    #we compute the % of the correct predictions\n",
    "    acc = ((RDD_check_result.reduce(lambda x,y: x + y)) / n_rows)*100\n",
    "    return acc\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \n",
    "    y_hat = sigmoid(np.dot(w,X.T) + b)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** EXECUTION ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "filename = \"../alumno/Downloads/botnet_tot_syn_l.csv\"\n",
    "epochs = 10\n",
    "learning_r = 1.5\n",
    "epsilon = 0.000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- PROVIDED EXECUTION CODE ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0  ---->  LOSS:  1.0625990845510587  ACCURACY:  68.9594\n",
      "EPOCH 1  ---->  LOSS:  0.6132352741215287  ACCURACY:  80.0445\n",
      "EPOCH 2  ---->  LOSS:  0.42975863091405647  ACCURACY:  86.1113\n",
      "EPOCH 3  ---->  LOSS:  0.34496814055669  ACCURACY:  88.9956\n",
      "EPOCH 4  ---->  LOSS:  0.29916778329709814  ACCURACY:  90.3188\n",
      "EPOCH 5  ---->  LOSS:  0.2710746417751432  ACCURACY:  91.0819\n",
      "EPOCH 6  ---->  LOSS:  0.25222021785624027  ACCURACY:  91.671\n",
      "EPOCH 7  ---->  LOSS:  0.23873439111569686  ACCURACY:  92.0784\n",
      "EPOCH 8  ---->  LOSS:  0.22862425077199894  ACCURACY:  92.37729999999999\n",
      "EPOCH 9  ---->  LOSS:  0.2207656541650843  ACCURACY:  92.6129\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ea2c10832710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = readFile(filename)\n",
    "\n",
    "# standardize\n",
    "data_norm = normalize(data)\n",
    "\n",
    "w, b = train(data_norm, epochs, learning_r)\n",
    "acc = accuracy(w, b, data_norm)\n",
    "\n",
    "print(\"acc: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
